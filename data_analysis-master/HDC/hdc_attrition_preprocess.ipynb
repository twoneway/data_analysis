{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/ml/algorithmlabs.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(f'{path}/*.csv')\n",
    "name = pd.read_excel('ml/feature_korean_name.xlsx', sheet_name='attrition')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAAV_BASEINFO_DETAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_list[4])\n",
    "bt = pd.read_csv(file_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 세팅 영역으로 21년 1~12월의 재직자 데이터를 참고했을 때, 6개월 후(22년 1~6월)에 누가 퇴사할 위험성이 높은지 예측\n",
    "bt_data_train = bt[(bt['DT_JOIN']<=20221231) & ((bt['DT_RETIRE'] >= 20220101) | (bt['DS_RETIRE'] == '재직'))]\n",
    "bt_data_test = bt[(bt['DT_JOIN']<=20221231) & ((bt['DT_RETIRE'] >= 20230101) | (bt['DS_RETIRE'] == '재직'))]\n",
    "bt_data_train['TEMP'] = 'train'\n",
    "bt_data_test['TEMP'] = 'test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bt_data = pd.concat([bt_data_train, bt_data_test], axis=0)\n",
    "bt_data = bt_data.reset_index(drop=True)\n",
    "bt_data['DT_YEAR'] = 2022\n",
    "bt_data['ID_SABUN'] = bt_data['ID_SABUN'].astype('str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_data = bt_data[[\"TEMP\", \"ID_SABUN\", \"DT_YEAR\", \"DT_GROUPJOIN\", \"DT_JOIN\", \"CD_JOBTYPE\", \"CD_POSITION\", \"CD_DEPT\", \"CD_BONBU\", \"TY_DEPT\", \"DT_RETIRE\", \"DS_RETIRE\", \"CD_OCCUPATION\", \"CD_DUTY\", \n",
    "                   \"DT_BIRTH\", \"DS_BIRTHPLACE\", \"CD_ADOPTYPE\", \"YN_HANDICAP\", \"YN_SUPPORTED\", \"YN_FOREIGNER\", \"YN_GENDER\", \"CD_JOBFAMILY\", \"CD_JOBFAMILY_JOIN\", \n",
    "                   \"DS_JOBTYPE\", \"DS_POSITION\", \"DS_DEPT\", \"DS_BONBU\", \"DS_TY_DEPT\", \"DS_OCCUPATION\", \"DS_DUTY\", \"DS_ADOPTYPE\", \"DS_JOBFAMILY\", \"DS_JOBFAMILY_JOIN\",\n",
    "                   'DS_HNAME', 'DS_HANDPHONE', 'DS_EMAIL']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DACT_USEDVACATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_list[5])\n",
    "vat = pd.read_csv(file_list[5])\n",
    "vat_data = vat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vat_data = vat_data[vat_data['YN_COMMONVACATION']!='Y']\n",
    "vat_data.sort_values(by=['ID_SABUN', 'DT_VACAFROM'], ascending=True, inplace=True)\n",
    "vat_data = vat_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vat_list = []\n",
    "\n",
    "for id in vat_data['ID_SABUN'].unique().tolist() : \n",
    "    cnt = 1\n",
    "    temp_vat = vat_data[vat_data['ID_SABUN']==id]\n",
    "    temp_vat = temp_vat.reset_index(drop=True)\n",
    "    for j in range(1, len(temp_vat)) : \n",
    "        first = datetime(int(str(temp_vat['DT_VACAFROM'][j-1])[0:4]), int(str(temp_vat['DT_VACAFROM'][j-1])[4:6]), int(str(temp_vat['DT_VACAFROM'][j-1])[6:8]))\n",
    "        second = datetime(int(str(temp_vat['DT_VACAFROM'][j])[0:4]), int(str(temp_vat['DT_VACAFROM'][j])[4:6]), int(str(temp_vat['DT_VACAFROM'][j])[6:8]))\n",
    "        if (first + timedelta(days=1))==second : \n",
    "            cnt += 1\n",
    "        else : \n",
    "            vat_list.append([id, temp_vat['DT_VACAFROM'][j-1], cnt])\n",
    "            cnt = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacation = pd.DataFrame(vat_list, columns=['ID_SABUN', 'DT_VACAFROM', 'VAT'])\n",
    "vacation = vacation[vacation['VAT']==1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = []\n",
    "month = []\n",
    "for i in range(len(vacation)) : \n",
    "    year.append(str(vacation['DT_VACAFROM'][i])[0:4])\n",
    "    month.append(str(vacation['DT_VACAFROM'][i])[4:6])\n",
    "    \n",
    "vacation['DT_YEAR'] = year\n",
    "vacation['DT_MONTH'] = month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = []\n",
    "\n",
    "for id in vacation['ID_SABUN'].unique().tolist() : \n",
    "    temp_data = vacation[vacation['ID_SABUN']==id]\n",
    "    value_list = temp_data[temp_data['DT_YEAR']=='2022']['DT_MONTH'].value_counts()\n",
    "    for month, cnt in zip (value_list.index, value_list.values) :\n",
    "        date_list.append([id, 2022, month, cnt])\n",
    "        \n",
    "used_data = pd.DataFrame(date_list, columns=['ID_SABUN', 'DT_YEAR', 'DT_MONTH', 'VAT_CNT'])\n",
    "used_data['ID_SABUN'] = used_data['ID_SABUN'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_month = ['04', '05', '06']\n",
    "inf_month = ['10', '11', '12']\n",
    "\n",
    "vat_month = []\n",
    "\n",
    "for i in range(len(bt_data)) : \n",
    "    vat_month_list = [] \n",
    "    if bt_data['TEMP'][i] == 'train' : \n",
    "        temp_data = used_data[used_data['ID_SABUN']==bt_data['ID_SABUN'][i]]\n",
    "        for month in train_month : \n",
    "            if month in temp_data['DT_MONTH'].unique().tolist() : \n",
    "                vat_month_list.append(temp_data[temp_data['DT_MONTH']==month]['VAT_CNT'].values[0])\n",
    "            else : \n",
    "                vat_month_list.append(0)\n",
    "    else : \n",
    "        temp_data = used_data[used_data['ID_SABUN']==bt_data['ID_SABUN'][i]]\n",
    "        for month in inf_month : \n",
    "            if month in temp_data['DT_MONTH'].unique().tolist() : \n",
    "                vat_month_list.append(temp_data[temp_data['DT_MONTH']==month]['VAT_CNT'].values[0])\n",
    "            else : \n",
    "                vat_month_list.append(0)\n",
    "    vat_month.append(vat_month_list)\n",
    "\n",
    "bt_data[['VAT_THREE_MONTH_AGO', 'VAT_TWO_MONTH_AGO', 'VAT_ONE_MONTH_AGO']] = vat_month"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAJT_FINAL_RESULT, DAKT_MBO_USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_list[8])\n",
    "wpe = pd.read_csv(file_list[8])\n",
    "wpe_data = wpe.copy()\n",
    "wpe_data = wpe_data[(wpe_data['CD_CORP']=='A101') & (wpe_data['DT_YEAR']>=2018)]\n",
    "\n",
    "print(file_list[9])\n",
    "mbo = pd.read_csv(file_list[9])\n",
    "mbo_data = mbo.copy()\n",
    "mbo_data = mbo_data[(mbo_data['CD_CORP']=='A101') & (mbo_data['DT_YEAR']>=2018)]\n",
    "\n",
    "wpe_data = wpe_data[['ID_SABUN', 'DT_YEAR', 'NO_POINT_TOTAL_FIN', 'DS_POINT_TOTAL_FIN']]\n",
    "mbo_data = mbo_data[['CD_USER', 'DT_YEAR', 'NO_FINAL_POINT', 'DS_FINAL_POINT']]\n",
    "mbo_data.rename(columns = {'CD_USER' : 'ID_SABUN'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_data = pd.merge(wpe_data, mbo_data, on=['ID_SABUN', 'DT_YEAR'], how='outer')\n",
    "point_data.dropna(subset = ['NO_POINT_TOTAL_FIN', 'DS_POINT_TOTAL_FIN', 'NO_FINAL_POINT', 'DS_FINAL_POINT'], axis=0, how='all', inplace=True)\n",
    "point_data['ID_SABUN'] = point_data['ID_SABUN'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_years = []\n",
    "\n",
    "for i in range(len(bt_data)) : \n",
    "    temp_list = []\n",
    "    temp_data = point_data[point_data['ID_SABUN']==bt_data['ID_SABUN'][i]]\n",
    "    main = bt_data['DT_YEAR'][i] - 1\n",
    "    if main not in temp_data['DT_YEAR'].unique().tolist() : \n",
    "        temp_list.append(0)\n",
    "        temp_list.append(0)\n",
    "    else : \n",
    "        if ((temp_data[temp_data['DT_YEAR']==main]['NO_POINT_TOTAL_FIN'].values[0]==temp_data[temp_data['DT_YEAR']==main]['NO_POINT_TOTAL_FIN'].values[0]) |\n",
    "        (temp_data[temp_data['DT_YEAR']==main]['DS_POINT_TOTAL_FIN'].values[0]==temp_data[temp_data['DT_YEAR']==main]['DS_POINT_TOTAL_FIN'].values[0])) : \n",
    "            temp_list.append(temp_data[temp_data['DT_YEAR']==main]['NO_POINT_TOTAL_FIN'].values[0])\n",
    "            temp_list.append(temp_data[temp_data['DT_YEAR']==main]['DS_POINT_TOTAL_FIN'].values[0])\n",
    "        else : \n",
    "            temp_list.append(temp_data[temp_data['DT_YEAR']==main]['NO_FINAL_POINT'].values[0])\n",
    "            temp_list.append(temp_data[temp_data['DT_YEAR']==main]['DS_FINAL_POINT'].values[0])  \n",
    "            \n",
    "    one_years.append(temp_list) \n",
    "    \n",
    "bt_data[['FINAL_POINT_RECENT_YEAR', 'FINAL_GRADE_RECENT_YEAR']] = one_years"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DACT_WORK_RESULT_UPDATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_list[6])\n",
    "work = pd.read_csv(file_list[6])\n",
    "work_data = work.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = []\n",
    "month = [] \n",
    "for i in range(len(work_data)) : \n",
    "    year.append(int(str(work_data['DT_VACAFROM'][i])[0:4]))\n",
    "    month.append(int(str(work_data['DT_VACAFROM'][i])[4:6]))\n",
    "    \n",
    "work_data['DT_YEAR'] = year\n",
    "work_data['DT_MONTH'] = month\n",
    "work_data['ID_SABUN'] = work_data['ID_SABUN'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_result = [] \n",
    "\n",
    "for i in range(len(bt_data)) : \n",
    "    work_month = [0, 0, 0, 0, 0, 0]\n",
    "    if bt_data['TEMP'][i] == 'train' : \n",
    "        temp_data = work_data[work_data['ID_SABUN']==bt_data['ID_SABUN'][i]]\n",
    "        point_list = temp_data['DT_MONTH'].value_counts()\n",
    "        for point in point_list.index : \n",
    "            if point <= 6 : \n",
    "                work_month[point-1] = point_list[point]\n",
    "    else : \n",
    "        temp_data = work_data[work_data['ID_SABUN']==bt_data['ID_SABUN'][i]]\n",
    "        point_list = temp_data['DT_MONTH'].value_counts()\n",
    "        for point in point_list.index : \n",
    "            if point >= 7 : \n",
    "                work_month[point-7] = point_list[point]\n",
    "            \n",
    "    work_result.append(work_month)\n",
    "    \n",
    "bt_data[['WEAK_ONE', 'WEAK_TWO', 'WEAK_THREE', 'WEAK_FOUR', 'WEAK_FIVE', 'WEAK_SIX']] = work_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAET_EDUCATION_REQUEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_list[7])\n",
    "edu = pd.read_csv(file_list[7])\n",
    "edu_data = edu.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_list = []\n",
    "\n",
    "for id in edu_data['ID_SABUN'].unique().tolist() : \n",
    "    first = 0\n",
    "    second = 0\n",
    "    temp_data = edu_data[edu_data['ID_SABUN']==id]\n",
    "    for i in range(len(temp_data)) : \n",
    "        if int(str(temp_data['DT_START'].values[i])[4:6]) in [1, 2, 3, 4, 5, 6] : \n",
    "            first += 1 \n",
    "        else : \n",
    "            second += 1\n",
    "    edu_list.append([id, first, second])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_temp = pd.DataFrame(edu_list, columns=['ID_SABUN', 'FIRST', 'SECOND'])\n",
    "edu_temp['ID_SABUN'] = edu_temp['ID_SABUN'].astype('str')\n",
    "\n",
    "edu_cnt = []\n",
    "for i in range(len(bt_data)) : \n",
    "    if bt_data['ID_SABUN'][i] not in edu_temp['ID_SABUN'].unique().tolist() : \n",
    "        edu_cnt.append(0)\n",
    "    else : \n",
    "        if bt_data['TEMP'][i] == 'train' : \n",
    "            edu_cnt.append(edu_temp[edu_temp['ID_SABUN']==bt_data['ID_SABUN'][i]]['FIRST'].values[0])\n",
    "        else : \n",
    "            edu_cnt.append(edu_temp[edu_temp['ID_SABUN']==bt_data['ID_SABUN'][i]]['SECOND'].values[0])\n",
    "        \n",
    "bt_data['EDU_CNT'] = edu_cnt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = bt_data[bt_data['TEMP']=='train']\n",
    "\n",
    "label_list = []\n",
    "\n",
    "for i in range(len(missing_data)) : \n",
    "    if (missing_data['DS_RETIRE'][i] == '재직') : \n",
    "        label_list.append(0)\n",
    "    else : \n",
    "        if missing_data['DT_RETIRE'][i] > 20220630 and missing_data['DT_RETIRE'][i] < 20230101 : \n",
    "            label_list.append(1)\n",
    "        elif missing_data['DT_RETIRE'][i] >= 20230101 :\n",
    "            label_list.append(0)\n",
    "        else : \n",
    "            label_list.append(-1)\n",
    "    \n",
    "    \n",
    "missing_data['LABEL'] = label_list\n",
    "\n",
    "missing_data = missing_data[missing_data['LABEL']!=-1]\n",
    "missing_data = missing_data[missing_data['DT_JOIN'] <= 20220630]\n",
    "\n",
    "missing_data.drop(['ID_SABUN', 'DT_RETIRE', 'DS_RETIRE', 'FINAL_POINT_RECENT_YEAR', 'TEMP', 'DT_YEAR', \n",
    "                   'DS_JOBTYPE', 'DS_POSITION', 'DS_DEPT', 'DS_BONBU', 'DS_OCCUPATION', 'DS_DUTY', 'DS_ADOPTYPE', 'DS_JOBFAMILY', 'DS_JOBFAMILY_JOIN', 'DS_TY_DEPT',\n",
    "                   'DS_HNAME', 'DS_HANDPHONE', 'DS_EMAIL'], axis=1, inplace=True)\n",
    "missing_rate = (missing_data.isnull().sum()/len(missing_data)).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_data['CD_JOBTYPE'] = bt_data['CD_JOBTYPE'].fillna(bt_data['CD_JOBTYPE'].mode()[0])\n",
    "bt_data['CD_JOBFAMILY'] = bt_data['CD_JOBFAMILY'].fillna(bt_data['CD_JOBFAMILY'].mode()[0])\n",
    "bt_data['CD_DUTY'] = bt_data['CD_DUTY'].fillna(bt_data['CD_DUTY'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_data['YN_HANDICAP'] = bt_data['YN_HANDICAP'].fillna('N')\n",
    "bt_data['YN_SUPPORTED'] = bt_data['YN_SUPPORTED'].fillna('N')\n",
    "bt_data['YN_FOREIGNER'] = bt_data['YN_FOREIGNER'].fillna('N')\n",
    "bt_data['YN_GENDER'] = bt_data['YN_GENDER'].fillna('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ch_grade(data, columns) : \n",
    "    data[columns] = data[columns].replace({'O' : 3, 'R' : 2, 'D' : 1, 0 : 2, 'A' : 3, 'B' : 2, 'C' : 1})\n",
    "    data[columns] = data[columns].fillna(2)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_grade(bt_data, 'FINAL_GRADE_RECENT_YEAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_change = {' ' : '서울', '대한민국' : '서울', '충청남도' : '충남', '광주광역시' : '광주', ' 서울' : '서울', '강원도' : '강원', '상주' : '경북', '구미' : '경북',\n",
    "                '충남 서산' : '충남', '경기도 성남시' : '경기', '수원' : '경기', '한국' : '서울', '파라과이' : '해외', '대한민국' : '서울', '전북 익산' : '전북', \n",
    "                '강릉' : '강원', '경북 대구' : '대구', '포항' : '경북', '청주' : '충북', '경기도 안양시' : '경기', '부산광역시' : '부산', '전북 전주' : '전북',\n",
    "                '전남 광주' : '광주', '경남 통영시' : '경남', '경상남도' : '경남', '경기도 수원시' : '경기', '경기 광명' : '경기', '강원도 홍천' : '강원',\n",
    "                '경북 영덕군' : '경북', ' 충주 ' : '충북', '경기도 수원' : '경기', '인천광역시' : '인천', '서울특별시' : '서울', '전북 남원' : '전북'}\n",
    "\n",
    "bt_data['DS_BIRTHPLACE'].replace(birth_change, inplace=True)\n",
    "bt_data['DS_BIRTHPLACE'] = bt_data['DS_BIRTHPLACE'].fillna(bt_data['DS_BIRTHPLACE'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = bt_data[bt_data['TEMP']=='train']\n",
    "inference_data = bt_data[bt_data['TEMP']=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature : 20220630 / label : 20230101 / v4\n",
    "# feature : 20220630 / label : 20221001 / v5\n",
    "# feature : 20220331 / label : 20220630 / v6\n",
    "# feature : 20220930 / label : 20230101 / v7\n",
    "\n",
    "label_list = []\n",
    "\n",
    "for i in range(len(train_data)) : \n",
    "    if (train_data['DS_RETIRE'][i] == '재직') : \n",
    "        label_list.append(0)\n",
    "    else : \n",
    "        if train_data['DT_RETIRE'][i] > 20220630 and train_data['DT_RETIRE'][i] < 20230101 : \n",
    "            label_list.append(1)\n",
    "        elif train_data['DT_RETIRE'][i] >= 20230101 :\n",
    "            label_list.append(0)\n",
    "        else : \n",
    "            label_list.append(-1)\n",
    "    \n",
    "    \n",
    "train_data['LABEL'] = label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[train_data['LABEL']!=-1]\n",
    "train_data = train_data[train_data['DT_JOIN'] <= 20220630]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(['DT_RETIRE', 'DS_RETIRE', 'FINAL_POINT_RECENT_YEAR', 'TEMP', 'DT_YEAR', 'DS_HNAME', 'DS_HANDPHONE', 'DS_EMAIL'], axis=1, inplace=True)\n",
    "inference_data.drop(['DT_RETIRE', 'DS_RETIRE', 'FINAL_POINT_RECENT_YEAR', 'TEMP', 'DT_YEAR'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = {}\n",
    "for i in range(len(name)) : \n",
    "    name_dict[name['feature'][i]] = name['feature_name'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today().strftime(\"%Y%m%d\")\n",
    "Path(f\"/ml/algorithmlabs.inference/{today}/attrition/pickle\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_data.to_csv(f\"/ml/algorithmlabs.inference/{today}/attrition/train_data_attrition.csv\", index=False)\n",
    "inference_data.to_csv(f\"/ml/algorithmlabs.inference/{today}/attrition/inference_data_attrition.csv\", index=False)\n",
    "joblib.dump(missing_rate, f\"/ml/algorithmlabs.inference/{today}/attrition/pickle/missing_rate.pkl\")\n",
    "joblib.dump(name_dict, f\"/ml/algorithmlabs.inference/{today}/attrition/pickle/name_dict.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "055ce310dfd61a9649730d91fa4dbaceeaf5df2d807db0aa8f4c64267c1c77f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
