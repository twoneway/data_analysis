{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, StratifiedKFold\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score, roc_curve, make_scorer, f1_score\n",
    "from datetime import datetime\n",
    "import shap\n",
    "import json\n",
    "import joblib\n",
    "from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'c:\\\\Users\\\\wongi\\\\Desktop\\\\알고리즘랩스\\\\프로젝트\\\\현대산업개발\\\\develop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(f'{path}\\\\algorithmlabs.inference\\\\train_data_promotion.csv')\n",
    "missing_rate = joblib.load(f'{path}\\\\algorithmlabs.inference\\\\pickle_promotion\\\\missing_rate.pkl')\n",
    "name_dict = joblib.load(f'{path}\\\\algorithmlabs.inference\\\\pickle_promotion\\\\name_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['NO_POINT_SUM'] = round(raw_data['NO_POINT_SUM'], 1)\n",
    "train = raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['YN_HANDICAP'].replace({'Y' : 1, 'N' : 0}, inplace=True)\n",
    "train['YN_SUPPORTED'].replace({'Y' : 1, 'N' : 0}, inplace=True)\n",
    "train['YN_FOREIGNER'].replace({'Y' : 1, 'N' : 0}, inplace=True)\n",
    "train['YN_GENDER'].replace({'M' : 1, 'W' : 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train['DS_BIRTHPLACE'] = le.fit_transform(train['DS_BIRTHPLACE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['DS_BONBU', 'DS_DEPT', 'DS_JOBFAMILY', 'DS_ZONE', 'DS_JOBTYPE', 'DS_DUTY', 'DS_ADOPTYPE', 'DS_JOBFAMILY_JOIN'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0411 수정\n",
    "params = setup(data=train, target='LABEL', train_size=0.75, fold=10, session_id=0, preprocess=True, fix_imbalance=True, \n",
    "               numeric_features=train.drop(['LABEL'], axis=1).columns.tolist(), ignore_features=['ID_SABUN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pull()\n",
    "trainset_size = results.data['Value'][results.data[results.data['Description']=='Transformed Train Set'].index[0]][0]\n",
    "testset_size = results.data['Value'][results.data[results.data['Description']=='Transformed Test Set'].index[0]][0]\n",
    "total_size = trainset_size + testset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0411 수정\n",
    "today = datetime.today().strftime(\"%Y%m%d\")\n",
    "top1 = compare_models(n_select=1, sort='Recall', include=['lightgbm', 'rf', 'dt', 'gbc', 'et', 'catboost', 'xgboost'])\n",
    "save_model(top1 , 'promotion_model')\n",
    "saved_model = load_model('promotion_model')\n",
    "predict = predict_model(saved_model.named_steps[\"trained_model\"])\n",
    "metrics = pull()\n",
    "\n",
    "metrics_dict = {}\n",
    "metrics_dict['accuracy_score'] = metrics.head(1)['Accuracy'].values[0]\n",
    "metrics_dict['recall_score'] = metrics.head(1)['Recall'].values[0]\n",
    "metrics_dict['precision_score'] = metrics.head(1)['Prec.'].values[0]\n",
    "metrics_dict['f1_score'] = metrics.head(1)['F1'].values[0]\n",
    "metrics_dict['auc'] = metrics.head(1)['AUC'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipe = saved_model[:-1].transform(train)\n",
    "explainer = shap.TreeExplainer(saved_model.named_steps[\"trained_model\"])\n",
    "shap_values = explainer.shap_values(train_pipe)\n",
    "\n",
    "if len(shap_values) == 2 :\n",
    "    importances = np.absolute(shap_values[1]).sum(axis=0) / shap_values[1].shape[0] \n",
    "else : \n",
    "    importances = np.absolute(shap_values).sum(axis=0) / shap_values.shape[0]\n",
    "    \n",
    "feature_importance = pd.Series(importances / np.sum(importances))\n",
    "feature_importance.index = train.drop(['ID_SABUN', 'LABEL'], axis=1).columns\n",
    "fe_dict = feature_importance.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fe_dict = {}\n",
    "\n",
    "for name in fe_dict.keys() : \n",
    "    for name_ex in name_dict.keys() : \n",
    "        if name == name_ex : \n",
    "            new_fe_dict[name_dict[name_ex]] = fe_dict[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_missing_dict = {}\n",
    "\n",
    "for name in missing_rate.keys() : \n",
    "    for name_ex in name_dict.keys() : \n",
    "        if name == name_ex : \n",
    "            new_missing_dict[name_dict[name_ex]] = missing_rate[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dict = {}\n",
    "main_dict['feature_importance'] = new_fe_dict\n",
    "main_dict['trainset_size'] = trainset_size\n",
    "main_dict['testset_size'] = testset_size\n",
    "main_dict['total_size'] = total_size\n",
    "main_dict['last_train_date'] = today\n",
    "main_dict['predict_semester'] = '2023년'\n",
    "main_dict['data_period'] = '2020-01-01 ~ 2022-12-31'\n",
    "main_dict['metrics'] = metrics_dict\n",
    "main_dict['feature_missing_rate'] = new_missing_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'{path}\\\\algorithmlabs.inference\\\\model_data_promotion.json'\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(json.dumps(main_dict, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(saved_model, f'{path}\\\\algorithmlabs.inference\\\\pickle_promotion\\\\model.pkl')\n",
    "joblib.dump(explainer, f'{path}\\\\algorithmlabs.inference\\\\pickle_promotion\\\\explainer.pkl')\n",
    "joblib.dump(le, f'{path}\\\\algorithmlabs.inference\\\\pickle_promotion\\\\label_encoder.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
