{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 사용한 패키지 목록 정리 <br>\n",
    "    - 추천 시스템에 자주 사용되는 surprise 패키지를 통해 추천 리스트 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from surprise import KNNBasic, SVD, BaselineOnly\n",
    "from surprise.model_selection import LeaveOneOut\n",
    "from reco_utils.common.general_utils import invert_dictionary \n",
    "#trainset을 DataFrame으로 확인하는 패키지\n",
    "# 설치 방법 : pip install pre-reco-utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 로컬에서 데이터셋 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hrd_raw = pd.read_csv(r'C:\\현대차R&D 추천 알고리즘 데이터_210425.csv')\n",
    "hrd_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrd_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hrd_raw.nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hrd_raw.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Surprise 패키지 형식에 맞도록 데이터셋 업로드 <br>\n",
    "    - 실제 학습, 실험 데이터로 활용하기 위해서는 'user-item-rating'의 데이터 형식을 맞춰주어야 surprise 패키지 활용 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5)) #rating_scale은 1~5점 척도\n",
    "hrd_data = Dataset.load_from_df(hrd_raw[['User_ID', 'Item_ID', 'Rating']], reader) \n",
    "# user,item, rating 순으로 rating 점수가 5점척도의 데이터셋 형태로 surprise 패키지가 읽음 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 사용할 알고리즘 정리 <br>\n",
    "    - SVD : Singular Value Decomposition 의 준말로 임의의 m x n 차원의 행렬 A 에 대하여 행렬을 분해하는 방법으로 특이값 분해라고 부름 <br>\n",
    "    - 행렬을 대각화하여 분해한 후 user, item, factor 라는 그룹을 만들어 추천 데이터 생성에 필요한 데이터를 최소화하고 비어있는 user들의 <br> \n",
    "    평점을 예측하여 추천 시스템에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = SVD(n_factors = 500, n_epochs = 50, init_mean = 0, init_std_dev = 0.1, lr_all = 0.005, reg_all = 0.02, random_state=1)\n",
    "# n_factors : 요인의 개수(몇 개 요인으로 그룹을 나눌 것인지), n_epochs : SGD를 몇번 반복할 것인지, init_mean : 정규분포 평균 \n",
    "# init_std_dev : 정규분포 표준편차, lr_all : 학습률, reg_all : 정규화, random_state : 랜덤시드(난수)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 데이터셋 분할 <br>\n",
    "    - 추천 알고리즘의 특성상 testset에 있는 유저들을 올바르게 평가하기 위해서는 학습 데이터에도 동일 유저들에 대한 평가기록이 <br> 존재해야 하는 문제점이 있음 <br>\n",
    "    - 따라서 testset에 user들의 점수가 반드시 1개 포함되도록 설정하여 원활한 학습이 이루어지도록 함 <br>\n",
    "    - 1개 과목만 학습한 유저들의 정보는 무시하여 학습 데이터를 활용 -> 1개만 학습한 인원들을 trainset에 넣고 싶지만 매커니즘상 <br>\n",
    "    test 상에 포함되도록 하여 이슈가 발생해 무시하는 방향으로 최종 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = LeaveOneOut(n_splits=5, random_state=1, min_n_ratings=1)\n",
    "# LeaveOneOut : testset에 반드시 각각의 user들의 점수가 1개 포함되도록 하는 함수 \n",
    "# n_splits : 5개의 fold로 나누기\n",
    "# min_n_ratings = 1 : trainset에 반드시 최소 1개 이상 user의 rating 점수가 포함되어야 한다라는 조건"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trainset, testset in kf.split(hrd_data):\n",
    "    \n",
    "    # train and test algorithm\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    accuracy.rmse(predictions, verbose=True)\n",
    "    accuracy.mae(predictions, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-1. 분할된 데이터셋 시각화(나누어진 데이터셋을 표로 시각화하여 보는 것이기에 필요성은 떨어짐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surprise_trainset_to_df(trainset, col_user=\"uid\", col_item=\"iid\", col_rating=\"rating\"): \n",
    "    df = pd.DataFrame(trainset.all_ratings(), columns=[col_user, col_item, col_rating])\n",
    "    map_user = trainset._inner2raw_id_users if trainset._inner2raw_id_users is not None else invert_dictionary(trainset._raw2inner_id_users)\n",
    "    map_item = trainset._inner2raw_id_items if trainset._inner2raw_id_items is not None else invert_dictionary(trainset._raw2inner_id_items)\n",
    "    df[col_user] = df[col_user].map(map_user)\n",
    "    df[col_item] = df[col_item].map(map_item)\n",
    "    return df\n",
    "# Surprise 패키지는 분석 시 raw_data -> inner_data로 변환하여 분석(surprise에 더 적합한 데이터 형식으로 변경)\n",
    "# 따라서 직접 trainset을 visualize할 수 없어 이를 다시 raw_data로 변경하는 가공과정이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = surprise_trainset_to_df(trainset, col_user=\"uid\", col_item=\"iid\", col_rating=\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frame = pd.DataFrame(testset, columns=['uid', 'iid', 'rating'])\n",
    "test_frame\n",
    "# testset은 반환형식이 list라 별도의 변환없이 접근 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 추천 리스트 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_number = 16043 # User_ID 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6-1. user가 과거 학습했던 item 리스트 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_item = hrd_raw.loc[hrd_raw['User_ID']==id_number, ['Item_ID', 'Rating'], ] #학습했었던 item 목록 저장\n",
    "experience = pd.DataFrame(seen_item)\n",
    "experience.sort_values([\"Rating\"], ascending=[False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6-2. 추천 리스트 필터링\n",
    "\n",
    "조건1. 동일한 조직('실')의 인원들이 듣지 않은 과목명은 추천 리스트에서 제외 \n",
    "\n",
    "조건2. Category가 공통과정에 속한 과목명일 경우에는 추천 리스트에서 제외되지 않고 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommender_filtering_list(number): # user 아이디를 받아 해당 user의 추천 리스트를 예측하기 위한 과목명 리스트 반환 \n",
    "    group_data = hrd_raw.loc[hrd_raw['User_ID']==number, ['사업부', '실', '조직_담당', '직무명']] #user의 '실' 정보를 행에서 읽어오기\n",
    "    group_room = group_data['실'].values[0] # '실' 이름 저장\n",
    "    group_center = group_data['사업부'].values[0] # '사업부' 이름 저장\n",
    "    group_organization = group_data['조직_담당'].values[0]\n",
    "    group_job = group_data['직무명'].values[0]\n",
    "    # group_list = hrd_raw.loc[hrd_raw['사업부']==group_center, 'Item_ID'] # user의 '사업부'를 기준으로 해당 과목 읽어오기\n",
    "    group_list = hrd_raw.loc[hrd_raw['실']==group_room, 'Item_ID'] # user의 '실'을 기준으로 해당 과목 읽어오기\n",
    "    # group_list = hrd_raw.loc[hrd_raw['조직_담당']==group_organization, 'Item_ID'] # user의 '실'을 기준으로 해당 과목 읽어오기\n",
    "    # group_list = hrd_raw.loc[hrd_raw['직무명']==group_job, 'Item_ID'] # user의 '실'을 기준으로 해당 과목 읽어오기\n",
    "    \n",
    "    array_group_list = np.array(group_list) # Pandas Series -> Numpy Array\n",
    "    category_raw = hrd_raw.loc[hrd_raw['Category']=='공통과정', 'Item_ID'] # 공통과정 불러오기\n",
    "    category_data = np.array(category_raw) # Pandas Series -> Numpy Array\n",
    "    category_list = np.unique(category_data) # 중복된 데이터 제거 \n",
    "    \n",
    "    filtering_raw = np.concatenate((array_group_list, category_list), axis = 0) \n",
    "    # 동일 그룹의 인원들이 학습한 과목명 + 공통과정\n",
    "    filtering_data = np.unique(filtering_raw) # 중복된 데이터 제거\n",
    "    already_seen = hrd_raw.loc[hrd_raw['User_ID']==number, 'Item_ID'] # user가 과거 학습했었던 과목명 저장\n",
    "    filtering_predict = np.setdiff1d(filtering_data, already_seen) # 동일그룹 + 공통과정 - user가 기학습했던 과목\n",
    "    \n",
    "    return filtering_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6-3. user가 학습하지 않았던 과목명 중에서 추천 리스트 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender_list = []\n",
    "for iid in recommender_filtering_list(id_number):\n",
    "    recommender_list.append((iid, algo.predict(uid=id_number, iid=iid).est)) \n",
    "    # 예측을 진행한 후 인접한 이웃이 충분히 존재하지 않을 경우 predictions 값을 전체 평점의 평균을 출력하도록 설정되어 해당사항 유의\n",
    "    \n",
    "pd.DataFrame(recommender_list, columns=['iid', 'predictions']).sort_values('predictions', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Coverage 계산 <br>\n",
    "    - Coverage : 추천 시스템이 추천을 할 수 있는 아이템의 수로 얼마나 다양한 아이템을 추천할 수 있는지 측정하는 지표\n",
    "    - 데이터들의 경향성을 잘 반영해 학습된 추천 시스템의 경우 전체 아이템 중 소수의 아이템들만 추천할 가능성이 높아 <br>\n",
    "전체 아이템에서 얼마나 알맞게 잘 추천할 수 있는지가 중요\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_raw = hrd_raw['User_ID'] # user의 아이디 불러오기\n",
    "user_array = np.array(user_raw) # Series -> Array\n",
    "user_list = np.unique(user_array) # Pandas Series -> Numpy Array "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7-1. SVD Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_list = [] # 도출된 10개의 추천 리스트를 저장하는 리스트\n",
    "\n",
    "for uid in user_list : \n",
    "    predict_list = []\n",
    "    for iid in recommender_filtering_list(uid) : \n",
    "        predict_list.append((iid, algo.predict(uid=uid, iid=iid).est)) \n",
    "    sort_list = pd.DataFrame(predict_list, columns=['iid', 'predictions']).sort_values('predictions', ascending=False).head(10)\n",
    "    sort_values = sort_list['iid'].values\n",
    "    for values in sort_values : # 도출된 10개의 추천 리스트를 저장\n",
    "        coverage_list.append(values)\n",
    "        \n",
    "final_coverage_list = np.unique(coverage_list) # 저장된 리스트들의 중복을 제거하여 최종 coverage 리스트 도출\n",
    "final_coverage_list.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.2 KNNBasic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'name' : 'pearson', 'user_based' : False} # 사용하는 유사도 측정방식 : 피어슨, 아이템 기반 협력필터링 사용 \n",
    "algo_Knn = KNNBasic(sim_options=sim_options) # KNNBasic 알고리즘 사용\n",
    "for trainset_Knn, testset_Knn in kf.split(hrd_data):\n",
    "    \n",
    "    # train and test algorithm\n",
    "    algo_Knn.fit(trainset_Knn)\n",
    "    predictions_Knn = algo_Knn.test(testset_Knn)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    accuracy.rmse(predictions_Knn, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_Knn_list = [] # 도출된 10개의 추천 리스트를 저장하는 리스트\n",
    "\n",
    "for uid in user_list : \n",
    "    predict_list = []\n",
    "    for iid in recommender_filtering_list(uid) : \n",
    "        predict_list.append((iid, algo_Knn.predict(uid=uid, iid=iid).est)) \n",
    "    sort_list = pd.DataFrame(predict_list, columns=['iid', 'predictions']).sort_values('predictions', ascending=False).head(10)\n",
    "    sort_Knn_list = sort_list.loc[sort_list['predictions']!=trainset.global_mean, 'iid'] \n",
    "    # trainset.global_mean : global mean of all ratings\n",
    "    sort_Knn_array = np.array(sort_Knn_list)\n",
    "    for values in sort_Knn_array : \n",
    "        coverage_Knn_list.append(values)\n",
    "        \n",
    "final_coverage_Knn_list = np.unique(coverage_Knn_list) # 저장된 리스트들의 중복을 제거하여 최종 coverage 리스트 도출\n",
    "final_coverage_Knn_list.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.3 BaselineOnly Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsl_options = {'method': 'sgd', \n",
    "               'reg' : 0.02,\n",
    "               'learning_rate': 0.005,\n",
    "               'n_epochs' : 50\n",
    "               }\n",
    "algo_BO = BaselineOnly(bsl_options=bsl_options) \n",
    "# user, item 2개의 카테고리 값 입력에서 평점의 예측치를 예측하는 회귀분석모형\n",
    "# 사용자와 상품 특성에 의한 평균 평점의 합으로 계산\n",
    "for trainset_BO, testset_BO in kf.split(hrd_data):\n",
    "    \n",
    "    # train and test algorithm\n",
    "    algo_BO.fit(trainset_Knn)\n",
    "    predictions_BO = algo_BO.test(testset_Knn)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    accuracy.rmse(predictions_BO, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_BO_list = [] \n",
    "\n",
    "for uid in user_list : \n",
    "    predict_list = []\n",
    "    for iid in recommender_filtering_list(uid) : \n",
    "        predict_list.append((iid, algo_BO.predict(uid=uid, iid=iid).est)) \n",
    "    sort_list = pd.DataFrame(predict_list, columns=['iid', 'predictions']).sort_values('predictions', ascending=False).head(10)\n",
    "    sort_values = sort_list['iid'].values\n",
    "    for values in sort_values : \n",
    "        coverage_BO_list.append(values)\n",
    "        \n",
    "final_coverage_BO_list = np.unique(coverage_BO_list) \n",
    "final_coverage_BO_list.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SVD Coverage : 207\n",
    "\n",
    "2. KNNBasic Coverage : 163\n",
    "\n",
    "3. BaselineOnly Coverage : 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ndcg(surprise_predictions, k_highest_scores=None):\n",
    "    \"\"\" \n",
    "    Calculates the ndcg (normalized discounted cumulative gain) from surprise predictions, using sklearn.metrics.ndcg_score and scipy.sparse\n",
    "  \n",
    "    Parameters: \n",
    "    surprise_predictions (List of surprise.prediction_algorithms.predictions.Prediction): list of predictions\n",
    "    k_highest_scores (positive integer): Only consider the highest k scores in the ranking. If None, use all. \n",
    "  \n",
    "    Returns: \n",
    "    float in [0., 1.]: The averaged NDCG scores over all recommendations\n",
    "  \n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.metrics import ndcg_score\n",
    "    from scipy import sparse\n",
    "    \n",
    "    uids = [p.uid for p in surprise_predictions ]\n",
    "    iids = [p.iid for p in surprise_predictions ]\n",
    "    r_uis = [p.r_ui for p in surprise_predictions ]\n",
    "    ests = [p.est for p in surprise_predictions ]\n",
    "\n",
    "    uids_number_list = []\n",
    "    for a in range(len(uids)) :\n",
    "      uids_number_list.append(a)\n",
    "\n",
    "    item_list = np.unique(iids)\n",
    "    iids_number = []\n",
    "    for a in range(len(item_list)) : \n",
    "      iids_number.append(a)\n",
    "\n",
    "    unique_item = pd.DataFrame(item_list, columns=['item'])\n",
    "    unique_item['iids_number'] = iids_number\n",
    "\n",
    "    iids_number_list = []\n",
    "    for a in range(len(iids)) :\n",
    "      number = unique_item.loc[unique_item['item']==iids[a], 'iids_number'].values[0]\n",
    "      iids_number_list.append(number)\n",
    "    \n",
    "    sparse_preds = sparse.coo_matrix((ests, (uids_number_list, iids_number_list)))\n",
    "    sparse_vals = sparse.coo_matrix((r_uis, (uids_number_list, iids_number_list)))\n",
    "    \n",
    "    dense_preds = sparse_preds.toarray().reshape(1, -1)\n",
    "    dense_vals = sparse_vals.toarray().reshape(1, -1)\n",
    "    \n",
    "    return ndcg_score(y_true= dense_vals , y_score= dense_preds, k=k_highest_scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ndcg(predictions, k_highest_scores=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
