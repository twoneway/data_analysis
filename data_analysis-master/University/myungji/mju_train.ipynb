{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKF1rhTMMM_7"
      },
      "outputs": [],
      "source": [
        "!pip install category-encoders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from category_encoders import TargetEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, VotingClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score, roc_curve, make_scorer, f1_score\n",
        "from imblearn.pipeline import Pipeline\n",
        "from datetime import datetime\n",
        "import shap\n",
        "import json\n",
        "import joblib"
      ],
      "metadata": {
        "id": "qXrhLDQHMYiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = pd.read_csv('/ml/algorithmlabs.inference/train_data.csv')\n",
        "missing_rate = joblib.load('/ml/algorithmlabs.inference/pickle/missing_rate.pkl')\n",
        "name_dict = joblib.load('/ml/algorithmlabs.inference/pickle/name_dict.pkl')"
      ],
      "metadata": {
        "id": "OEqUvNYyMZNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = raw_data.copy()"
      ],
      "metadata": {
        "id": "p5ih9W_BMaJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0207 수정\n",
        "# 0208 수정\n",
        "train_data.drop(['STUDENT_CD', 'CHG_YEAR', 'CHG_SMT', 'BIRTH', '이름', 'CHG_DIV'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "MfXamHF2MbBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# target encoding\n",
        "object_cols = train_data.select_dtypes('object').columns\n",
        "te = TargetEncoder()\n",
        "train_data[object_cols] = te.fit_transform(train_data[object_cols], train_data['LABEL'])"
      ],
      "metadata": {
        "id": "pgI8lbTUMcBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split\n",
        "X = train_data.drop(['LABEL'], axis=1)\n",
        "y = train_data['LABEL']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify = y)"
      ],
      "metadata": {
        "id": "qJt5LRrWMc2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scaling\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_scaled = scaler.transform(X_train)\n",
        "X_train = pd.DataFrame(data=X_scaled, index=X_train.index, columns=X_train.columns)\n",
        "X_scaled = scaler.transform(X_test)\n",
        "X_test = pd.DataFrame(data=X_scaled, index=X_test.index, columns=X_test.columns)"
      ],
      "metadata": {
        "id": "P4PJD5KwMddf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# smote\n",
        "sm = SMOTE(random_state=0, k_neighbors=7)\n",
        "X_res, y_res = sm.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "p7aHM3BSMeJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline model\n",
        "model_rf = RandomForestClassifier(random_state=0)\n",
        "model_xg = XGBClassifier(random_state=0, use_label_encoder=False, objective='binary:logistic')\n",
        "model_ex = ExtraTreesClassifier(random_state=0)\n",
        "model_gb = GradientBoostingClassifier(random_state=0)"
      ],
      "metadata": {
        "id": "FuipLTlbMeQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
      ],
      "metadata": {
        "id": "mIYYpetHMeTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gridsearchcv\n",
        "params = [{'n_estimators':[200, 300], 'max_depth':[5]}]\n",
        "grid_rf = GridSearchCV(model_rf, param_grid=params, cv=kfold, scoring='recall', n_jobs=-1)\n",
        "grid_ex = GridSearchCV(model_ex, param_grid=params, cv=kfold, scoring='recall', n_jobs=-1)\n",
        "\n",
        "params_b = [{'n_estimators':[200, 300], 'max_depth':[5], 'learning_rate' : [.01, .001, .0001]}]\n",
        "grid_xg = GridSearchCV(model_xg, param_grid=params_b, cv=kfold, scoring='recall', n_jobs=-1)\n",
        "grid_gb = GridSearchCV(model_gb, param_grid=params_b, cv=kfold, scoring='recall', n_jobs=-1)\n",
        "\n",
        "grid_rf.fit(X_res, y_res)\n",
        "grid_xg.fit(X_res, y_res)\n",
        "grid_ex.fit(X_res, y_res)\n",
        "grid_gb.fit(X_res, y_res)\n",
        "\n",
        "best_rf = grid_rf.best_estimator_\n",
        "best_xg = grid_xg.best_estimator_\n",
        "best_ex = grid_ex.best_estimator_\n",
        "best_gb = grid_gb.best_estimator_"
      ],
      "metadata": {
        "id": "woIymdUMMeV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voting = VotingClassifier(estimators=[('rf', best_rf), ('xgb', best_xg), ('ex', best_ex), ('gb', best_gb)], voting='soft', n_jobs=-1)\n",
        "\n",
        "voting.fit(X_res, y_res)\n",
        "now = datetime.now()\n",
        "\n",
        "results_pred = voting.predict(X_test)\n",
        "acc = accuracy_score(y_test, results_pred)\n",
        "recall = recall_score(y_test, results_pred)\n",
        "pre = precision_score(y_test, results_pred)\n",
        "f1 = f1_score(y_test, results_pred)\n",
        "auc = roc_auc_score(y_test, results_pred)\n",
        "\n",
        "metrics_dict = {}\n",
        "metrics_dict['accuracy_score'] = acc\n",
        "metrics_dict['recall_score'] = recall\n",
        "metrics_dict['precision_score'] = pre\n",
        "metrics_dict['f1_score'] = f1\n",
        "metrics_dict['auc'] = auc"
      ],
      "metadata": {
        "id": "khyI45t7MeYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer_1 = shap.TreeExplainer(voting.estimators_[0], X_res, check_additivity=False, model_output='probability') \n",
        "shap_values_1 = explainer_1.shap_values(X_test, check_additivity=False) \n",
        "explainer_2 = shap.TreeExplainer(voting.estimators_[1], X_res, check_additivity=False, model_output='probability') \n",
        "shap_values_2 = explainer_2.shap_values(X_test, check_additivity=False) \n",
        "explainer_3 = shap.TreeExplainer(voting.estimators_[2], X_res, check_additivity=False, model_output='probability') \n",
        "shap_values_3 = explainer_3.shap_values(X_test, check_additivity=False) \n",
        "explainer_4 = shap.TreeExplainer(voting.estimators_[3], X_res, check_additivity=False, model_output='probability') \n",
        "shap_values_4 = explainer_4.shap_values(X_test, check_additivity=False) "
      ],
      "metadata": {
        "id": "3gV1Ov_eMebe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances_1 = np.absolute(shap_values_1[1]).sum(axis=0) / shap_values_1[1].shape[0]\n",
        "importances_2 = np.absolute(shap_values_2).sum(axis=0) / shap_values_2.shape[0]\n",
        "importances_3 = np.absolute(shap_values_3[1]).sum(axis=0) / shap_values_3[1].shape[0]\n",
        "importances_4 = np.absolute(shap_values_4).sum(axis=0) / shap_values_4.shape[0]\n",
        "\n",
        "importances_sum = (importances_1 + importances_2 + importances_3 + importances_4)/4\n",
        "\n",
        "feature_importance = pd.Series(importances_sum / np.sum(importances_sum))\n",
        "feature_importance.index = X_test.columns\n",
        "\n",
        "fe_dict = feature_importance.to_dict()"
      ],
      "metadata": {
        "id": "tOFb7x_jMeeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_fe_dict = {}\n",
        "\n",
        "for name in fe_dict.keys() : \n",
        "    for name_ex in name_dict.keys() : \n",
        "        if name == name_ex : \n",
        "            new_fe_dict[name_dict[name_ex]] = fe_dict[name]"
      ],
      "metadata": {
        "id": "LjAYGTCQMj5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_missing_dict = {}\n",
        "\n",
        "for name in missing_rate.keys() : \n",
        "    for name_ex in name_dict.keys() : \n",
        "        if name == name_ex : \n",
        "            new_missing_dict[name_dict[name_ex]] = missing_rate[name]"
      ],
      "metadata": {
        "id": "sKd49uwOMj8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_dict = {}\n",
        "main_dict['feature importance'] = new_fe_dict\n",
        "main_dict['trainset_size'] = len(X_train)\n",
        "main_dict['testset_size'] = len(X_test)\n",
        "main_dict['total_size'] = len(X_train)+len(X_test)\n",
        "main_dict['last_train_date'] = str(now)\n",
        "main_dict['predict_semester'] = '2023년 1학기'\n",
        "main_dict['data_period'] = '2021-03-01 ~ 2021-08-31'\n",
        "main_dict['metrics'] = metrics_dict\n",
        "main_dict['feature missing rate'] = new_missing_dict"
      ],
      "metadata": {
        "id": "lf0TZxV8Mj-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/ml/algorithmlabs.inference/model_data_dropout.json'\n",
        "with open(file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(json.dumps(main_dict, ensure_ascii=False, indent=2))"
      ],
      "metadata": {
        "id": "Qh1OsNEAMkBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(voting, '/ml/algorithmlabs.inference/pickle/model.pkl')\n",
        "joblib.dump(te, '/ml/algorithmlabs.inference/pickle/encoder.pkl')\n",
        "joblib.dump(scaler, '/ml/algorithmlabs.inference/pickle/scaler.pkl')\n",
        "joblib.dump(explainer_1, '/ml/algorithmlabs.inference/pickle/shap_explainer_1.pkl')\n",
        "joblib.dump(explainer_2, '/ml/algorithmlabs.inference/pickle/shap_explainer_2.pkl')\n",
        "joblib.dump(explainer_3, '/ml/algorithmlabs.inference/pickle/shap_explainer_3.pkl')\n",
        "joblib.dump(explainer_4, '/ml/algorithmlabs.inference/pickle/shap_explainer_4.pkl')"
      ],
      "metadata": {
        "id": "70EZfmRjMnDO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}