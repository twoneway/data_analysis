# -*- coding: utf-8 -*-
"""THETIS Video Classification(RGB+Skeleton)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rcOpYMwkUVOlKhMCJr5WZlnCfCYSUeFB
"""

# youtube video를 다루기 위한 package 설치
!pip install pafy youtube-dl moviepy

# 현재 youtube package 및 API에 대한 수정이 이루어지지 않는 이슈로 유튜브 영상 불러오기 시 에러 발생
# '싫어요'와 같이 삭제된 기능이 아직 API에 남아 있어 존재하지 않는 정보에 대해 충돌 발생
# 하단의 재설치하는 구조로 문제 해결
!pip uninstall -y pafy
!pip install git+https://github.com/Cupcakus/pafy

# 영상 프레임 위 skeleton을 그리기 위한 package 설치
!pip install mediapipe

# Commented out IPython magic to ensure Python compatibility.
import os
import cv2
import math
import pafy
import random
import numpy as np
import datetime as dt
import tensorflow as tf
import mediapipe as mp
from moviepy.editor import *
from collections import deque
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model

# THETIS 학습 데이터로 테니스 동작을 촬영한 데이터로 구성
!wget -nc --no-check-certificate http://thetis.image.ece.ntua.gr/databases/VIDEO_RGB.zip
!unzip VIDEO_RGB.zip

# 이미지 크기 및 최대 프레임 개수 설정
image_height, image_width = 64, 64
max_images_per_class = 8000

# 데이터셋 위치 파악
dataset_directory = "VIDEO_RGB"
classes_list = ['backhand', 'forehand_flat', 'flat_service']

model_output_size = len(classes_list)

# mediapipe 내 Pose와 관련된 솔루션 불러오기(학습된 모델)
mp_pose = mp.solutions.pose

# Pose 파라미터 설정
pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.3, model_complexity=2)

# mediapipe 내 그리기 도구 불러오기 
mp_drawing = mp.solutions.drawing_utils 

# mediapipe 내 스타일 불러오기
mp_drawing_styles = mp.solutions.drawing_styles

# 프레임 위에 skeleton을 그리고 해당 영상을 저장하는 함수
def draw_skeleton_video(video_file_path, output_file_path):

    # Reading the Video File using the VideoCapture Object
    video_reader = cv2.VideoCapture(video_file_path)

    # Getting the width and height of the video 
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Writing the Overlayed Video Files Using the VideoWriter Object
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 24, (original_video_width, original_video_height))

    with mp_pose.Pose(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5) as pose:
        while video_reader.isOpened():
            success, image = video_reader.read()
            if not success:
                break

            # 필요에 따라 성능 향상을 위해 이미지 작성을 불가능함으로 기본 설정
            image.flags.writeable = False
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            results = pose.process(image)

            # 포즈 주석을 이미지 위에 그립니다.
            image.flags.writeable = True
            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
            mp_drawing.draw_landmarks(
                image,
                results.pose_landmarks,
                mp_pose.POSE_CONNECTIONS,
                landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())

            video_writer.write(image)

        video_reader.release()
        video_writer.release()

input_video_file_path = "VIDEO_RGB/flat_service/p15_serflat_s3.avi"

# Creating The Output directories if it does not exist
output_directory = 'Youtube_Videos'
os.makedirs(output_directory, exist_ok = True)

output_video_file_path = f'{output_directory}/sample_data.avi'

# Calling the predict_on_live_video method to start the Prediction.
draw_skeleton_video(input_video_file_path, output_video_file_path)

# Play Video File in the Notebook
VideoFileClip(output_video_file_path).ipython_display(width = 700)

# skeleton 영상저장 경로 설정
saved_directory = 'Skeleton'
folders = [f'{saved_directory}/forehand_flat', f'{saved_directory}/backhand', f'{saved_directory}/flat_service']

for path in folders :
  os.makedirs(path, exist_ok=True)

# skeleton 영상 저장
# '동작명_번호' 파일명으로 저장
for folder in classes_list : 
  main_path = f'{dataset_directory}/{folder}'
  file_list = os.listdir(main_path)
  cnt = 1
  for file in file_list : 
    input_video_file_path = f'{main_path}/{file}'
    output_video_file_path = f'{saved_directory}/{folder}/{folder}_{cnt}.avi'
    draw_skeleton_video(input_video_file_path, output_video_file_path)
    cnt+=1

# 설정경로에 파일이 정상적으로 저장되었는지 확인
for path in folders : 
  file_count = len(os.listdir(path))
  print(file_count)

"""Modelling"""

# 데이터셋을 고정시키기 위한 random seed 설정
seed_constant = 23
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)

# 영상 프레임을 추출하여 리스트로 저장하기
# openCV는 기본적으로 이미지를 Numpy 배열 형태로 저장 
def frames_extraction(video_path):
    # Empty List declared to store video frames
    frames_list = []
    
    # Reading the Video File Using the VideoCapture
    video_reader = cv2.VideoCapture(video_path)

    # Iterating through Video Frames
    while True:

        # Reading a frame from the video file 
        success, frame = video_reader.read() 

        # If Video frame was not successfully read then break the loop
        if not success:
            break

        # Resize the Frame to fixed Dimensions
        resized_frame = cv2.resize(frame, (image_height, image_width))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        
        # Appending the normalized frame into the frames list
        frames_list.append(normalized_frame)
    
    # Closing the VideoCapture object and releasing all resources. 
    video_reader.release()

    # returning the frames list 
    return frames_list

def create_dataset():

    # Declaring Empty Lists to store the features and labels values.
    temp_features = [] 
    features = []
    labels = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(classes_list):
        print(f'Extracting Data of Class: {class_name}')
        
        # Getting the list of video files present in the specific class name directory
        files_list = os.listdir(os.path.join(saved_directory, class_name))

        # Iterating through all the files present in the files list
        for file_name in files_list:

            # Construct the complete video path
            video_file_path = os.path.join(saved_directory, class_name, file_name)

            # Calling the frame_extraction method for every video file path
            frames = frames_extraction(video_file_path)

            # Appending the frames to a temporary list.
            temp_features.extend(frames)
        
        # Adding randomly selected frames to the features list
        # 전체 샘플보다 추출하려는 k값이 클 경우 에러 발생하기에 max_images_per_class 값 조절 필요
        # 1개 클래스에 대한 영상 프레임을 모두 저장한 temp_features에서 max_image만큼의 sample 프레임을 일부 추출
        features.extend(random.sample(temp_features, max_images_per_class))

        # Adding Fixed number of labels to the labels list / max_image 개수만큼의 label 생성
        labels.extend([class_index] * max_images_per_class)
        
        # Emptying the temp_features list so it can be reused to store all frames of the next class.
        print(len(temp_features))
        temp_features.clear()

    # Converting the features and labels lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels

def modified_create_dataset():

    # Declaring Empty Lists to store the features and labels values.
    temp_features = [] 
    features = []
    labels = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(classes_list):
        print(f'Extracting Data of Class: {class_name}')
        cnt = 0

        # Getting the list of video files present in the specific class name directory
        files_list = os.listdir(os.path.join(saved_directory, class_name))

        # Iterating through all the files present in the files list
        for file_name in files_list:

            # Construct the complete video path
            video_file_path = os.path.join(saved_directory, class_name, file_name)

            # Calling the frame_extraction method for every video file path
            frames = frames_extraction(video_file_path)

            # Appending the frames to a temporary list.
            temp_features.extend(frames)

            # 50번째 영상까지 추출한 후 종료 
            cnt += 1
            if cnt == 25 :
              break
        
        # Adding randomly selected frames to the features list
        # 추출한 전체 프레임을 feature로 활용
        features.extend(temp_features)

        # Adding Fixed number of labels to the labels list / feature 개수만큼 label 생성(0, 1, 2)
        labels.extend([class_index] * len(temp_features))
        
        # Emptying the temp_features list so it can be reused to store all frames of the next class.
        print(len(temp_features))
        temp_features.clear()

    # Converting the features and labels lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels

features, labels = modified_create_dataset()

print(len(features))
print(len(labels))

one_hot_encoded_labels = to_categorical(labels)

features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.3, shuffle = True, random_state = seed_constant)

# Let's create a function that will construct our model
def create_model():

    # We will use a Sequential model for model construction
    model = Sequential()

    # Defining The Model Architecture
    model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu', input_shape = (image_height, image_width, 3)))
    model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size = (2, 2)))
    model.add(GlobalAveragePooling2D())
    model.add(Dense(256, activation = 'relu'))
    model.add(BatchNormalization())
    model.add(Dense(model_output_size, activation = 'softmax'))

    # Printing the models summary
    model.summary()

    return model


# Calling the create_model method
model = create_model()

print("Model Created Successfully!")

# Adding the Early Stopping Callback to the model which will continuously monitor the validation loss metric for every epoch.
# If the models validation loss does not decrease after 15 consecutive epochs, the training will be stopped and the weight which reported the lowest validation loss will be retored in the model.
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)

# Adding loss, optimizer and metrics values to the model.
model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start Training / 모델 학습하기
model_training_history = model.fit(x = features_train, y = labels_train, epochs = 50, batch_size = 4 , shuffle = True, validation_split = 0.3, callbacks = [early_stopping_callback])

# 모델 평가하기
model_evaluation_history = model.evaluate(features_test, labels_test)

# 다중 분류 모델의 confusion matrix를 그리는 함수 생성
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]),   range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")
        
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

from sklearn.metrics import confusion_matrix
import itertools

# 예측결과 도출 및 matrix 그리기(정규화X)
pred = model.predict(features_test)
pred_classes = np.argmax(pred, axis=1)
true = np.argmax(labels_test, axis=1)
confusion_mtx = confusion_matrix(true, pred_classes)
plot_confusion_matrix(confusion_mtx, classes=classes_list)

# matrix 그리기(정규화O)
plot_confusion_matrix(confusion_mtx, normalize=True, classes=classes_list)

# 학습 데이터와 검증 데이터의 성능비교 metric 함수
def plot_metric(metric_name_1, metric_name_2, plot_name):
  # Get Metric values using metric names as identifiers
  metric_value_1 = model_training_history.history[metric_name_1]
  metric_value_2 = model_training_history.history[metric_name_2]

  # Constructing a range object which will be used as time 
  epochs = range(len(metric_value_1))
  
  # Plotting the Graph
  plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)
  plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)
  
  # Adding title to the plot
  plt.title(str(plot_name))

  # Adding legend to the plot
  plt.legend()

plot_metric('loss', 'val_loss', 'Total Loss vs Total Validation Loss')

plot_metric('accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')

# youtube 영상을 다운로드 및 저장하고 해당 영상제목을 반환하는 함수
def download_youtube_videos(youtube_video_url, output_directory):
    # Creating a Video object which includes useful information regarding the youtube video.
    video = pafy.new(youtube_video_url)

    # Getting the best available quality object for the youtube video.
    video_best = video.getbest()

    # Constructing the Output File Path
    output_file_path = f'{output_directory}/{video.title}.mp4'

    # Downloading the youtube video at the best available quality.
    video_best.download(filepath = output_file_path, quiet = True)

    # Returning Video Title
    return video.title

# 학습한 모델을 활용하여 실제 예측(다중분류)을 진행해보는 함수
def predict_on_live_video(video_file_path, output_file_path, window_size):

    # Initialize a Deque Object with a fixed size which will be used to implement moving/rolling average functionality.
    predicted_labels_probabilities_deque = deque(maxlen = window_size)

    # Reading the Video File using the VideoCapture Object
    video_reader = cv2.VideoCapture(video_file_path)

    # Getting the width and height of the video / 원본 영상 사이즈 측정
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Writing the Overlayed Video Files Using the VideoWriter Object / 비디오 쓰기
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 24, (original_video_width, original_video_height))

    while True: 

        # Reading The Frame
        status, frame = video_reader.read() 

        if not status:
            break

        # Resize the Frame to fixed Dimensions
        resized_frame = cv2.resize(frame, (image_height, image_width))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255

        # Passing the Image Normalized Frame to the model and receiving Predicted Probabilities.
        predicted_labels_probabilities = model.predict(np.expand_dims(normalized_frame, axis = 0))[0]

        # Appending predicted label probabilities to the deque object
        predicted_labels_probabilities_deque.append(predicted_labels_probabilities)

        # Assuring that the Deque is completely filled before starting the averaging process
        if len(predicted_labels_probabilities_deque) == window_size:

            # Converting Predicted Labels Probabilities Deque into Numpy array
            predicted_labels_probabilities_np = np.array(predicted_labels_probabilities_deque)

            # Calculating Average of Predicted Labels Probabilities Column Wise 
            predicted_labels_probabilities_averaged = predicted_labels_probabilities_np.mean(axis = 0)

            # Converting the predicted probabilities into labels by returning the index of the maximum value.
            predicted_label = np.argmax(predicted_labels_probabilities_averaged)

            # calculate predicted_value
            predicted_value = np.max(predicted_labels_probabilities_averaged)

            # Accessing The Class Name using predicted label.
            predicted_class_name = classes_list[predicted_label]

            predicted_text = predicted_class_name + " : " + str(predicted_value)
          
            # Overlaying Class Name Text Ontop of the Frame
            cv2.putText(frame, predicted_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

        # Writing The Frame
        video_writer.write(frame)

    # Closing the VideoCapture and VideoWriter objects and releasing all resources held by them. 
    video_reader.release()
    video_writer.release()

# 다운로드한 파일 저장경로 설정 및 유튜브 영상 다운로드
# Creating The Output directories if it does not exist
output_directory = 'Youtube_Videos'
os.makedirs(output_directory, exist_ok = True)

def visualize_predict(youtube_url, start, end, window_size) : 
    # Downloading a YouTube Video
    video_title = download_youtube_videos(youtube_url, output_directory)

    # Getting the YouTube Video's path you just downloaded
    input_video_file_path = f'{output_directory}/{video_title}.mp4'

    # Extracting the subclip duration beacuse of the purpose
    clip = VideoFileClip(input_video_file_path)
    clip = clip.subclip(start, end)
    subclip_video_file_path = f'{output_directory}/{video_title}_subclip.mp4'
    clip.write_videofile(subclip_video_file_path)

    # drawing the skeleton on the video
    result_video_file_path = f'{output_directory}/{video_title}_result.mp4'
    draw_skeleton_video(subclip_video_file_path, result_video_file_path)

    # Construting The Output YouTube Video Path
    output_video_file_path = f'{output_directory}/{video_title} -Output-WSize {window_size}.mp4'

    # Calling the predict_on_live_video method to start the Prediction.
    predict_on_live_video(result_video_file_path, output_video_file_path, window_size)

    # Play Video File in the Notebook / 비디오 영상 플레이(구역별 클립을 따서 영상송출 가능)
    new_clip = VideoFileClip(output_video_file_path)
    return new_clip.ipython_display(width = 700)

visualize_predict('https://www.youtube.com/watch?v=R1Ho2JZqVOE', 10, 40, 25)

visualize_predict('https://www.youtube.com/watch?v=Q67lXOqWAdE', 31, 51, 25)

# 각 Label 클래스별 예측값을 도출하는 함수
def make_average_predictions(video_file_path, predictions_frames_count):
    
    # Initializing the Numpy array which will store Prediction Probabilities
    predicted_labels_probabilities_np = np.zeros((predictions_frames_count, model_output_size), dtype = np.float)

    # Reading the Video File using the VideoCapture Object
    video_reader = cv2.VideoCapture(video_file_path)

    # Getting The Total Frames present in the video 
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculating The Number of Frames to skip Before reading a frame
    skip_frames_window = video_frames_count // predictions_frames_count

    for frame_counter in range(predictions_frames_count): 

        # Setting Frame Position
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading The Frame
        _ , frame = video_reader.read() 

        # Resize the Frame to fixed Dimensions
        resized_frame = cv2.resize(frame, (image_height, image_width))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255

        # Passing the Image Normalized Frame to the model and receiving Predicted Probabilities.
        predicted_labels_probabilities = model.predict(np.expand_dims(normalized_frame, axis = 0))[0]

        # Appending predicted label probabilities to the deque object
        predicted_labels_probabilities_np[frame_counter] = predicted_labels_probabilities

    # Calculating Average of Predicted Labels Probabilities Column Wise 
    predicted_labels_probabilities_averaged = predicted_labels_probabilities_np.mean(axis = 0)

    # Sorting the Averaged Predicted Labels Probabilities
    predicted_labels_probabilities_averaged_sorted_indexes = np.argsort(predicted_labels_probabilities_averaged)[::-1]

    # Iterating Over All Averaged Predicted Label Probabilities
    for predicted_label in predicted_labels_probabilities_averaged_sorted_indexes:

        # Accessing The Class Name using predicted label.
        predicted_class_name = classes_list[predicted_label]

        # Accessing The Averaged Probability using predicted label.
        predicted_probability = predicted_labels_probabilities_averaged[predicted_label]

        print(f"CLASS NAME: {predicted_class_name}   AVERAGED PROBABILITY: {(predicted_probability*100):.2}")
    
    # Closing the VideoCapture Object and releasing all resources held by it. 
    video_reader.release()

def visualize_predict_metrics(youtube_url, start, end) : 

    # Downloading The YouTube Video
    video_title = download_youtube_videos(youtube_url, output_directory)

    # Construting The Input YouTube Video Path
    input_video_file_path = f'{output_directory}/{video_title}.mp4'

    # Extracting the subclip duration beacuse of the purpose
    clip = VideoFileClip(input_video_file_path)
    clip = clip.subclip(start, end)
    subclip_video_file_path = f'{output_directory}/{video_title}_subclip.mp4'
    clip.write_videofile(subclip_video_file_path)

    # drawing the skeleton on the video
    result_video_file_path = f'{output_directory}/{video_title}_result.mp4'
    draw_skeleton_video(subclip_video_file_path, result_video_file_path)

    # Calling The Make Average Method To Start The Process
    make_average_predictions(result_video_file_path, 50)

    # Play Video File in the Notebook
    clip = VideoFileClip(result_video_file_path)
    return clip.ipython_display(width = 700)

visualize_predict_metrics('https://www.youtube.com/watch?v=amLoMd4yt0k', 10, 30)